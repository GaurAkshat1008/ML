{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print('Length of the text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def text_to_int(text):\n",
    "  return np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "print('{} ---- characters mapped to int ---- > {}'.format(text[:13], text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_text(ints):\n",
    "  try:\n",
    "    ints = ints.numpy()\n",
    "  except:\n",
    "    pass\n",
    "  return ''.join(idx2char[ints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n"
     ]
    }
   ],
   "source": [
    "print(int_to_text(text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "  input_text = chunk[:-1]\n",
    "  target_text = chunk[1:]\n",
    "  return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Example \n",
      "\n",
      "INPUT\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "TARGET\n",
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "\n",
      " Example \n",
      "\n",
      "INPUT\n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you \n",
      "TARGET\n",
      "re all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataset.take(2):\n",
    "  print('\\n\\n Example \\n')\n",
    "  print(\"INPUT\")\n",
    "  print(int_to_text(x))\n",
    "  print(\"TARGET\")\n",
    "  print(int_to_text(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab) # number of unique chraracters\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# buffer size to shuffle the data\n",
    "# ( tf data is designed to work with possibly infinte sequences, so it doesn't attempt to shuffle the entire sequence in memory. Instead it maintains a buffer in memory and shuffles the buffer)\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_unit, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_unit, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           16640     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "  example_batch_prediction = model(input_example_batch)\n",
    "  print(example_batch_prediction.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tf.Tensor(\n",
      "[[[ 1.16113387e-03  6.71223504e-04 -1.53272611e-03 ... -1.20814855e-03\n",
      "   -3.56934033e-03  2.85322219e-03]\n",
      "  [-2.30107550e-03  3.62346997e-03 -5.52291749e-03 ...  3.24174849e-04\n",
      "    8.66473885e-04  3.82824708e-03]\n",
      "  [ 3.00625805e-03  2.07547680e-03 -6.77848700e-04 ...  1.22490362e-03\n",
      "   -9.59064346e-05  3.71046667e-03]\n",
      "  ...\n",
      "  [ 1.02552613e-02  3.70247383e-03  3.10393446e-03 ... -3.72546259e-03\n",
      "   -7.75642134e-03 -2.38509523e-03]\n",
      "  [ 4.43634810e-03  9.57961567e-03 -1.34826847e-03 ...  5.31708170e-03\n",
      "   -7.40646198e-03 -5.30722504e-03]\n",
      "  [ 8.70527886e-03  2.22264929e-03 -1.25982740e-03 ... -1.68588827e-03\n",
      "   -7.30965938e-03 -4.39319480e-03]]\n",
      "\n",
      " [[ 1.16113387e-03  6.71223504e-04 -1.53272611e-03 ... -1.20814855e-03\n",
      "   -3.56934033e-03  2.85322219e-03]\n",
      "  [ 5.75832557e-03  5.34478435e-03 -9.28938389e-05 ... -3.77976615e-03\n",
      "   -9.24482476e-04 -1.97509583e-03]\n",
      "  [ 4.60580410e-03 -6.40970701e-03 -6.41067652e-03 ... -3.29688448e-03\n",
      "    6.79670973e-03  3.07477359e-03]\n",
      "  ...\n",
      "  [ 4.45518177e-03  9.96608846e-03 -8.11575446e-03 ... -5.66711323e-03\n",
      "   -6.53558411e-04  4.68594953e-06]\n",
      "  [ 3.45100765e-03  7.88510218e-03 -9.81579721e-03 ... -8.02301057e-03\n",
      "   -5.01113478e-04 -7.99102243e-04]\n",
      "  [ 7.94873759e-03  5.04656089e-03 -9.91803221e-03 ... -1.03741111e-02\n",
      "   -2.21905299e-03  1.91669632e-03]]\n",
      "\n",
      " [[ 2.88734166e-03 -9.76855983e-04 -3.93053982e-03 ... -6.62246998e-03\n",
      "    2.96777405e-04  2.79551372e-03]\n",
      "  [ 6.13095611e-03  7.51903455e-04 -1.05465110e-03 ... -8.38564988e-03\n",
      "    6.31068484e-04 -1.01743499e-04]\n",
      "  [ 5.98669238e-03  1.36176078e-03 -2.96053803e-03 ... -7.99231697e-03\n",
      "   -3.06040957e-03  9.15201032e-04]\n",
      "  ...\n",
      "  [ 1.94851891e-03  8.80403258e-03 -9.58362408e-03 ... -6.07425626e-03\n",
      "   -2.11986294e-03 -3.69482953e-03]\n",
      "  [-9.25320201e-05  1.55344512e-03 -9.07700229e-03 ... -8.93112645e-03\n",
      "   -2.34297267e-03  4.06543259e-05]\n",
      "  [ 1.33272342e-03  1.70854398e-03 -1.01460461e-02 ... -4.49373899e-03\n",
      "   -2.50296341e-03  7.09369034e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 4.85775992e-03 -1.22052641e-03 -1.40084187e-03 ... -3.80205596e-03\n",
      "   -2.91977730e-03  1.88635290e-03]\n",
      "  [ 4.18012356e-03  8.33640574e-04 -7.92147359e-04 ... -7.74194486e-04\n",
      "    6.82402402e-04  7.14299921e-03]\n",
      "  [ 4.84965369e-03  1.97489047e-03 -1.33856875e-03 ... -5.09284146e-04\n",
      "   -3.40190809e-03  7.09211547e-03]\n",
      "  ...\n",
      "  [ 6.52659964e-03  3.12407827e-03 -1.18825864e-02 ...  1.13665155e-04\n",
      "   -4.32918873e-03  7.91222323e-04]\n",
      "  [ 4.98364493e-03  2.89031467e-03 -9.52479988e-03 ... -4.82271309e-04\n",
      "   -8.31978675e-03  4.47195070e-03]\n",
      "  [-2.86771380e-03  1.82156544e-03 -3.16565973e-03 ... -2.65023415e-03\n",
      "   -1.04144877e-02  3.40313860e-03]]\n",
      "\n",
      " [[ 4.35391767e-03  4.79440391e-03 -3.67831788e-03 ...  5.29058510e-03\n",
      "    2.55182385e-05 -2.39768135e-03]\n",
      "  [-3.38170305e-03  1.65333319e-03 -2.33029597e-03 ...  1.20656786e-03\n",
      "   -1.51202641e-03  7.71401683e-04]\n",
      "  [-1.98567100e-03  1.13402749e-03 -2.89712474e-03 ... -3.23965156e-04\n",
      "   -5.90048032e-03  4.09279065e-03]\n",
      "  ...\n",
      "  [ 7.79796205e-03  4.34178486e-03 -7.17275729e-03 ... -1.33881252e-03\n",
      "   -9.56865586e-03  2.10946705e-03]\n",
      "  [ 7.23176356e-03  4.81927209e-03 -5.91351977e-03 ...  6.05617650e-04\n",
      "   -9.59234219e-03  1.49223395e-03]\n",
      "  [ 9.56026278e-03  1.00919381e-02 -1.18139349e-02 ...  3.19827348e-04\n",
      "   -9.46509186e-03 -3.16289975e-03]]\n",
      "\n",
      " [[ 5.11621311e-03  1.08507294e-02 -3.73216392e-03 ...  2.28798203e-03\n",
      "    5.23512205e-03  4.18118108e-03]\n",
      "  [ 4.09088517e-03  4.47136909e-03 -1.18924668e-02 ... -1.05273677e-03\n",
      "    5.84190199e-03  1.48222258e-03]\n",
      "  [ 2.37919507e-03 -1.33596989e-03 -9.95423831e-03 ... -3.93013982e-03\n",
      "    2.89435871e-03  4.52694530e-03]\n",
      "  ...\n",
      "  [-8.30932753e-04  6.73550367e-03  1.19583169e-03 ... -7.29004713e-03\n",
      "   -3.80517892e-03  1.34511758e-03]\n",
      "  [ 5.49138524e-03  5.56429476e-03 -2.11899122e-03 ... -1.08984020e-02\n",
      "   -4.83038696e-03  2.78060278e-03]\n",
      "  [ 8.18625838e-03  3.93879693e-03  3.27455765e-03 ... -6.01702137e-03\n",
      "   -6.81274477e-03  3.30378348e-03]]], shape=(64, 100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(len(example_batch_prediction))\n",
    "print(example_batch_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"!PYCWuiEh;!CBmrFSF:ApgaY3OwSGjnoAd\\nwxohYfL!nH\\nvbtGT3wZf!LVMEv'YcWK\\nSzG&YyUKpp;fvy3EmETRF:VFvTBLp$3V:\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a loss function for 3d array\n",
    "pred = example_batch_prediction[0]\n",
    "\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
    "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
    "predicted_chars = int_to_text(sampled_indices)\n",
    "\n",
    "predicted_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "  filepath = checkpoint_prefix,\n",
    "  save_weights_only = True,\n",
    "  save_format = 'hdf5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "172/172 [==============================] - 977s 6s/step - loss: 2.5638\n",
      "Epoch 2/2\n",
      "172/172 [==============================] - 915s 5s/step - loss: 1.8642\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "history = model.fit(data, epochs=2, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('shakespeare_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('shakespare.h5', custom_objects={'loss': loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "new_model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x171be4dded0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_num = 2\n",
    "new_model.load_weights(checkpoint_prefix.format(epoch=checkpoint_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # low temperatures results in more predictable text.\n",
    "  # higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 0.6\n",
    "\n",
    "  # Here batch size == 1\n",
    "  new_model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "    predictions = model(input_eval)\n",
    "\n",
    "    # remove the batch dimension\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "    # using a categorical distribution to predict the character returned by the model\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "    # We pass the predicted character as the next input to the model\n",
    "    # along with the previous hidden state\n",
    "\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    text_generated.append(idx2char[predicted_id])\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "romeow.\n",
      "\n",
      "SICINIUS:\n",
      "O wive as not that she anger to tee this deeps to my formon where I line a will pay the russ deaven with his promy.\n",
      "\n",
      "KING RICHARD III:\n",
      "I prord you so slayer with misten this allace;\n",
      "And I will dight in the respised and the kings the dears is with hime\n",
      "And ears more, so dead the vonse, in the have my lord,\n",
      "And they he wan thou arm furse and strevest in the beed than may it well.\n",
      "Betsee thou nather and so more in thy shall with here\n",
      "stall with lively love, it itness.\n",
      "\n",
      "Sechard:\n",
      "Non sund so sot, you well me it father pantime for the robe, the good for with your down;\n",
      "Les sagh the see:\n",
      "I will the word look--pairs and for here brenger to-mand and man\n",
      "To will at stay the life so leter throw as like,\n",
      "And I am know they may the laweral fither or with his like a this\n",
      "To cranion of his soul not be resides,\n",
      "In see strow the gentlemant and the head will mather,\n",
      "I have seep the prochous to say the graces.\n",
      "\n",
      "ARTOUCHIO:\n",
      "I am the couls thee it instainst.\n",
      "\n",
      "LORTHUS:\n",
      "I will the lany?\n",
      "\n",
      "DUKE VI\n"
     ]
    }
   ],
   "source": [
    "inp = input('Type a starting string: ')\n",
    "print(generate_text(new_model, inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7401c3a6738d15d752740af9785731698b5ffd5adfb1a735c0bc94da9902075"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
