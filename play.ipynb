{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print('Length of the text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def text_to_int(text):\n",
    "  return np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = text_to_int(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "print('{} ---- characters mapped to int ---- > {}'.format(text[:13], text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_text(ints):\n",
    "  try:\n",
    "    ints = ints.numpy()\n",
    "  except:\n",
    "    pass\n",
    "  return ''.join(idx2char[ints])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen\n"
     ]
    }
   ],
   "source": [
    "print(int_to_text(text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "  input_text = chunk[:-1]\n",
    "  target_text = chunk[1:]\n",
    "  return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Example \n",
      "\n",
      "INPUT\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n",
      "TARGET\n",
      "irst Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You \n",
      "\n",
      "\n",
      " Example \n",
      "\n",
      "INPUT\n",
      "are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you \n",
      "TARGET\n",
      "re all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you k\n"
     ]
    }
   ],
   "source": [
    "for x, y in dataset.take(2):\n",
    "  print('\\n\\n Example \\n')\n",
    "  print(\"INPUT\")\n",
    "  print(int_to_text(x))\n",
    "  print(\"TARGET\")\n",
    "  print(int_to_text(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = len(vocab) # number of unique chraracters\n",
    "EMBEDDING_DIM = 256\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# buffer size to shuffle the data\n",
    "# ( tf data is designed to work with possibly infinte sequences, so it doesn't attempt to shuffle the entire sequence in memory. Instead it maintains a buffer in memory and shuffles the buffer)\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "data = dataset.shuffle(buffer_size=BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_unit, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_unit, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           16640     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,330,241\n",
      "Trainable params: 5,330,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in data.take(1):\n",
    "  example_batch_prediction = model(input_example_batch)\n",
    "  print(example_batch_prediction.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "tf.Tensor(\n",
      "[[[-3.6949608e-03  2.7353715e-04  3.2527572e-03 ... -2.8428072e-03\n",
      "    8.8533526e-04 -1.7859146e-04]\n",
      "  [ 5.4476303e-03 -4.1701952e-03 -2.3412150e-03 ...  2.2141607e-03\n",
      "    7.7489377e-03  9.2974445e-04]\n",
      "  [ 1.3644922e-02 -1.9484917e-03 -1.0778324e-03 ... -2.0351270e-03\n",
      "    7.3579066e-03  6.8059461e-03]\n",
      "  ...\n",
      "  [ 5.9976541e-03 -2.0526990e-03 -4.9554147e-03 ... -8.0194732e-04\n",
      "   -3.7999655e-04 -1.4247971e-03]\n",
      "  [ 8.5791312e-03 -1.8906562e-03  3.0837785e-03 ... -4.5740046e-03\n",
      "    2.8129914e-03 -3.7806341e-04]\n",
      "  [ 2.8496897e-03 -1.0516122e-04 -2.0917254e-03 ... -3.4135520e-03\n",
      "    4.9899817e-03 -4.3369168e-03]]\n",
      "\n",
      " [[ 3.1667808e-03 -1.1396809e-03  6.9900369e-03 ... -2.8581582e-03\n",
      "    3.1413180e-03  1.1248235e-03]\n",
      "  [-2.2382303e-03  3.3094781e-05  9.3078637e-04 ... -1.2569681e-03\n",
      "    5.4983636e-03 -2.8896974e-03]\n",
      "  [-1.0223915e-02  5.6589316e-03  4.4127153e-03 ... -3.5803139e-03\n",
      "   -3.1360688e-03 -3.3164080e-03]\n",
      "  ...\n",
      "  [ 1.1809905e-02 -1.7841556e-03 -9.2433468e-03 ...  4.0204655e-03\n",
      "    9.4169108e-03 -3.9941100e-03]\n",
      "  [ 5.8523593e-03 -2.2940207e-03 -9.0361368e-03 ...  2.5000107e-03\n",
      "    4.8025479e-03 -1.7149670e-03]\n",
      "  [ 5.6250663e-03 -3.8833476e-03 -4.0536299e-03 ...  1.3600362e-03\n",
      "    6.0765445e-03 -2.7819485e-03]]\n",
      "\n",
      " [[ 6.2729778e-05 -7.1341556e-04 -7.8573060e-04 ... -1.7818635e-03\n",
      "    4.2357747e-03  8.3342136e-04]\n",
      "  [ 4.2872094e-03 -2.6053018e-03 -7.0353650e-04 ... -4.2677708e-03\n",
      "    2.5685399e-03  5.2319560e-03]\n",
      "  [-2.9607578e-03  2.2467263e-03  3.8526901e-03 ... -6.8341494e-03\n",
      "   -2.6431074e-03  7.4089207e-03]\n",
      "  ...\n",
      "  [ 3.9690146e-03 -5.7120314e-03  5.2432776e-03 ... -1.0078954e-02\n",
      "    5.7197143e-03  3.1427043e-03]\n",
      "  [ 9.2395358e-03 -1.0754301e-02  1.0807235e-02 ... -6.0467292e-03\n",
      "    4.8159300e-03  7.9081794e-03]\n",
      "  [ 1.0456967e-02 -1.2710335e-02  2.1495083e-03 ... -5.0312681e-03\n",
      "   -1.4920841e-03  5.0584292e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.1016554e-03 -2.0228105e-04  5.1944493e-03 ... -1.5358857e-03\n",
      "   -2.6553727e-03 -6.6450615e-03]\n",
      "  [ 7.6707052e-03 -4.3854755e-03 -3.4896121e-04 ...  2.1687530e-03\n",
      "    4.9624327e-03 -4.0428676e-03]\n",
      "  [ 7.2809001e-03 -1.7110619e-03  1.1877378e-03 ...  1.3928282e-03\n",
      "    2.8445362e-04 -5.0733918e-03]\n",
      "  ...\n",
      "  [ 1.8251134e-02 -6.8965089e-03 -4.0895552e-03 ... -2.1114801e-03\n",
      "    1.7101336e-02 -4.3942966e-03]\n",
      "  [ 1.0470110e-02 -3.0031241e-03 -6.7040138e-03 ... -8.8429038e-04\n",
      "    1.7135689e-02 -7.1101720e-03]\n",
      "  [ 4.2985911e-03 -3.0343677e-03 -3.1484105e-03 ...  1.5601150e-03\n",
      "    1.4997398e-02  1.4266097e-03]]\n",
      "\n",
      " [[ 4.2995135e-03  3.0558622e-03  1.3238566e-03 ...  2.1405597e-03\n",
      "    2.8100298e-03 -1.5970818e-03]\n",
      "  [ 1.0035465e-02 -1.5382739e-03 -4.2645149e-03 ...  4.4908477e-03\n",
      "    9.1365566e-03  8.9288130e-04]\n",
      "  [ 1.6065340e-02  4.5971686e-04 -2.6681894e-03 ... -1.4214434e-03\n",
      "    8.3514247e-03  7.6142475e-03]\n",
      "  ...\n",
      "  [ 1.5117442e-02 -9.7334916e-03 -9.7273272e-03 ... -2.8658987e-03\n",
      "    1.0127981e-02  1.1745665e-02]\n",
      "  [ 1.3291691e-02 -7.4978746e-03 -1.4635056e-03 ... -4.5413184e-03\n",
      "    4.6901191e-03  1.7580247e-03]\n",
      "  [ 1.3134340e-02 -5.9163230e-03  1.3504389e-03 ... -8.2206447e-03\n",
      "   -7.3825149e-04 -1.6727886e-03]]\n",
      "\n",
      " [[ 8.6748069e-03  1.1386987e-03  6.4666721e-04 ... -4.1099996e-03\n",
      "    1.1127583e-03  6.5614013e-03]\n",
      "  [ 2.5020051e-03  1.5699321e-03 -3.8417378e-03 ... -9.8638400e-04\n",
      "    3.2104023e-03  2.7413594e-03]\n",
      "  [ 8.7323263e-03  6.8207213e-04 -8.7974593e-03 ... -4.0744590e-03\n",
      "    1.9012832e-03  7.8856181e-03]\n",
      "  ...\n",
      "  [ 8.6843129e-03 -5.4095201e-03  4.2159003e-03 ... -4.4325753e-03\n",
      "    1.2212987e-02  5.2289860e-03]\n",
      "  [ 1.4903748e-02 -8.5400017e-03 -2.0559113e-03 ... -6.3782246e-03\n",
      "    1.4060233e-02  3.0849709e-03]\n",
      "  [ 2.0437041e-02 -1.0889525e-02 -6.9507202e-03 ... -5.8952323e-04\n",
      "    1.7883383e-02  3.9800527e-03]]], shape=(64, 100, 65), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(len(example_batch_prediction))\n",
    "print(example_batch_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Vx?HqLveHCd\\n'V?Ns LBsDN.MxjOP?hi?X gCTkuqs'mW?e;ooNOxcV?rQBYuJRz3LExrwJ,!qDL,OmjDGXTmfyms.EzBTCllQq'\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a loss function for 3d array\n",
    "pred = example_batch_prediction[0]\n",
    "\n",
    "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
    "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
    "predicted_chars = int_to_text(sampled_indices)\n",
    "\n",
    "predicted_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "  filepath = checkpoint_prefix,\n",
    "  save_weights_only = True,\n",
    "  save_format = 'hdf5'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "172/172 [==============================] - 977s 6s/step - loss: 2.5638\n",
      "Epoch 2/2\n",
      "172/172 [==============================] - 915s 5s/step - loss: 1.8642\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "history = model.fit(data, epochs=2, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('shakespeare.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.keras.models.load_model('shakespare.h5', custom_objects={'loss': loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "new_model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x18d35f31690>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_num = 2\n",
    "new_model.load_weights(checkpoint_prefix.format(epoch=checkpoint_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # low temperatures results in more predictable text.\n",
    "  # higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 0.6\n",
    "\n",
    "  # Here batch size == 1\n",
    "  new_model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "    predictions = model(input_eval)\n",
    "\n",
    "    # remove the batch dimension\n",
    "    predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "    # using a categorical distribution to predict the character returned by the model\n",
    "    predictions = predictions / temperature\n",
    "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "    # We pass the predicted character as the next input to the model\n",
    "    # along with the previous hidden state\n",
    "\n",
    "    input_eval = tf.expand_dims([predicted_id], 0)\n",
    "    text_generated.append(idx2char[predicted_id])\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "romeow the knomes comntt there all best to mises, the seall dopress for my wishen,\n",
      "The follor saring your should some to the and cold hill the sirest for thay my sould meath of the pray;\n",
      "What the starest for beather,\n",
      "And by woll have for why srucaing bestinger in heart speather hear and to to the greal of the lody sterth not be the feather sof the death of here you rearsul there and reand to sto,\n",
      "When it the dines cain streeth pad the what bake he treest to cam stand tis bendest and made a not ane tood hither mare\n",
      "That not hear me in the proped and have all asterfower your warder the maret if thou dortay to wise to miser as of I besere to the song the\n",
      "\n",
      "AUTIO:\n",
      "Yit hath so hear the have mongh co thy loves of of here and the forsief wor her:\n",
      "The with for the come in sees the in to purest of the it com hour of herred being ther hank thou to thou dece deart.\n",
      "\n",
      "Prister:\n",
      "And with speer caksed her were of uppince the wall here om the sope,\n",
      "The will ta main to thou serel came of seeting my may your r\n"
     ]
    }
   ],
   "source": [
    "inp = input('Type a starting string: ')\n",
    "print(generate_text(new_model, inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7401c3a6738d15d752740af9785731698b5ffd5adfb1a735c0bc94da9902075"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
